\documentclass[]{IEEEtran}

% Your packages go here
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{amsmath}
%listings settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeblue}{rgb}{0,0.8,0.99}
\definecolor{codeyellow}{rgb}{0.6,0.5,0}


\lstdefinestyle{vim_like}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{codeyellow},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}
\lstset{style=vim_like}

\markboth{MO443 Digital Image Processing}{}

\begin{document}
  \title{Project 2 - Thresholding}
  \author{Thales Mateus Rodrigues Oliveira (RA 148051)
    \thanks{ra148051@students.ic.unicamp.br}
  }
  \maketitle
  
  \begin{abstract}
    In this project, it was given the task of applying global and local threshold methods to grayscale images. To fulfill the requirements, the solution implements 8 different algorithms, using approaches listed in the literature. It was able to generate the binarized thresholded images, and the differences of each approach, the pros and cons of the implementation are explained in this report.
  \end{abstract}
  
\section{Introduction}
Thresholding is a technique used for segmentation, and consists on the classification of the pixels within an image according to a(some) specific threshold(s). It is used for basic segmentation, to be able to separate objects from brackground in images and make it easier image processing steps. There are global techiniques, which considers one (or more) general threshold(s) for the entire picture, and local ones, that calculates local threshold(s) for every region in the image. The goal of this work is to implement global and local approaches, and be able to compare their strenghts and weakness. The next sections explains the implemented algorithm, the experiments realized and the output analysis
\par The code, along with the input files and the report is delivered in the compressed file THALES\_MATEUS\_RODRIGUES\_OLIVEIRA\_148051.tar, in the Google Classroom.
\section{The Program}
 
The program was implemented with Python 3.7.3. The libraries used and their respective versions are OpenCV 4.1.0, Numpy 1.16.4 and MatPlotLib 3.1.0.

\subsection{How to execute it}

The project has a Makefile available to help performing some actions on it. The Makefile has 3 basic instructions: clean, build and exec. Clean instruction removes generated images stored in the \textbf{output} folder, the execution code in \textbf{bin} folder and the folders itself. The Build instruction creates the \textbf{output} and \textbf{bin} folders, and moves the source code to \textbf{bin}. The Exec instruction executes the code with images in the \textbf{input} folder. Listing \ref{code:makefile} provides examples of how to execute the three instructions in a terminal.
\begin{lstlisting}[language=sh, caption={Makefile usage example}, label={code:makefile}]
  #clean environment, deletes output and bin folders and their content
  make clean
  
  #prepare the environment for code execution
  make build 

  #executes code
  make exec
\end{lstlisting}

\subsection{Input}

The program does not have an input argument by default, the input images are listed in code, and they are expected to be stored in the \textbf{input} folder. Listing \ref{code:input} shows how images are listed to be executed in code. The $images$ tuple is implemented in $src/main.py$.
\par The images are expected to be in the \textit{.pgm} format.

\begin{lstlisting}[language=Python, caption={Input images inside code}, label={code:input}]
  # for inserting other images, add tem to /input folder and list them here
  images = (
      'baboon',
      'fiducial',
      'monarch',
      'peppers',
      'retina',
      'sonnet',
      'wedge'
  )
\end{lstlisting}

\subsection{Output}
The output of the program is a series of binarized images based on the input ones, threshold methods and their peculiarities, and their respective histograms. The output images are stored in the \textbf{output} folder, and they stored based on global or local threshold methods (\textbf{global-thresholding} and \textbf{nxn-window} folders) and are labeled by concatenating the image name and the threshold method, for local cases (\textit{e.g.: output/99x99-window/baboon\_bernsen.pgm}). The histograms of the images are stored following the same convention (\textit{e.g.: output/99x99-window/fiducial\_contrast\_histogram.pgm}). The ratio of black (object) pixels in each image is also an output of the program, using the standard output. 

\subsection{Implementation}
The functions which implement the thresholding operations are defined in the \textit{src/thresholding.py} file. The file has an auxiliary function to build histograms for images \textit(calculate\_histogram), and has 8 thresholding functions, 1 for the global method and 7 other to the local approaches. The thresholding methods were implemented according to the project specification. For some local methods, as they deal with neighborhood centered in the analyzed pixel, the image is padded according to the neighborhood size. The convention here applied is that background pixels have maximum value (255) and object pixels have minimum value (0). The following items describe each of the used approaches.

\subsubsection{Global Thresholding}
The value of each image pixel is compared to a threshold. If greater than the threshold, it is an object pixel. Otherwise, it is background. It is the simpler method, and the used threshold is received as an input for the function. Listing \ref{code:global} shows the most important lines of the implementation.

\begin{lstlisting}[language=Python, caption={Global Thresholding Implementation}, label={code:global}]
def global_thresholding(img, threshold=128):
  ...
  result = np.where(img < threshold, 255, 0)
  result = np.uint8(result)
  ...
\end{lstlisting}

\subsubsection{Bernsen Thresholding}
The Bernsen approach for local thresholding calculates the threshold of a specific pixel based on the mean of the maximum and minimum values of its neighborhood centered in the pixel. Then, the comparison to set the value is done. Listing \ref{code:bernsen} shows the most important lines of the implementation.

\begin{lstlisting}[language=Python, caption={Bernsen Local Thresholding Implementation}, label={code:bernsen}]
  def bernsen_local_thresholding(img, window_size=3):
    ...
    padded_img = np.pad(img, (window_size//2, window_size//2), 'constant')
    for j in range(img_height):
        for i in range(img_width):
            window = padded_img[j:j + window_size, i:i + window_size]
            local_threshold = (int(np.min(window)) + int(np.max(window)))//2
            if img[j][i] < local_threshold:
                result[j][i] = 255
    ...
\end{lstlisting}

\subsubsection{Niblack Thresholding}
The Niblack approach for local thresholding is based on the pixel's neighborhood mean and standard deviation values. The threshold is calculated as:
\begin{equation}
  T(x, y) = \mu(x, y) + k\sigma(x,y) 
\end{equation}
where $\mu$,$\sigma$ are the mean and standard deviation in the neighborhood, respectively, and $k$ an adjustment factor. The literature calculates a good value of $k$ as $0.2$. Listing \ref{code:niblack} shows the main lines of the implementation.

\begin{lstlisting}[language=Python, caption={Niblack Local Thresholding Implementation}, label={code:niblack}]
  def niblack_local_thresholding(img, window_size=15, k=-0.2):
    ...
    for j in range(img_height):
        for i in range(img_width):
        window = img[window_size*(j//window_size):window_size*(j//window_size) + window_size,
        window_size*(i//window_size):window_size*(i//window_size) + window_size]
          mean = np.mean(window)
          std_dev = np.std(window)
          local_threshold = int(mean + k*std_dev)
          if img[j][i] < local_threshold:
              result[j][i] = 255
    ...
\end{lstlisting}

\subsubsection{Sauvola and Pietaksinen Thresholding}
This solution tries to improve Niblack one, specially for documents with bad lighting. The threshold is calculated as:

\begin{equation}
  T(x,y) = \mu (x,y)*(1 + k(\dfrac{\sigma(x,y)}{R} - 1))
\end{equation}

The standard deviation and mean are calculated based on neighborhood centered in the analyzed pixel. Listing \ref{code:sauvola} shows main parts of implementation. The default $k$ and $R$ values are suggestions from the creators.

\begin{lstlisting}[language=Python, caption={Sauvola and Pietaksinen Local Thresholding Implementation}, label={code:sauvola}]
  def sauvola_pietaksinen_local_thresholding(img, window_size=3, k=0.5, r=128):
    ...
    padded_img = np.pad(img, (window_size//2, window_size//2), 'constant')
    img_height, img_width = img.shape
    for j in range(img_height):
        for i in range(img_width):
            window = padded_img[j:j + window_size, i:i + window_size]
            mean = np.mean(window)
            std_dev = np.std(window)
            local_threshold = int(mean*(1 + k*((std_dev/r) - 1)))
            if img[j][i] < local_threshold:
                result[j][i] = 255
    ...
\end{lstlisting}

\subsubsection{Phansalskar, More and Sabale Thresholding}
This approach is a variation of Sauvola and Pietaksinen, to deal with low cotnrast images. The calculation of threshold for local neighborhood is done as:
\begin{equation}
  T(x, y) = \mu(x,y)*(1 + pexp(-q\mu(x,y)) + k*(\dfrac{\sigma(x,y)}{R} - 1))
\end{equation}

The $p, q, k, R$ default parameters are suggested from the authors as $2, 10, 0.25 and 0.5$, respectively. The calculation is done for normalized image. Listing \ref{code:more} shows the main aspects of the implementation. 

\begin{lstlisting}[language=Python, caption={Phansalskar, More and Sabale Local Thresholding Implementation}, label={code:more}]
  def phansalskar_more_sabale_local_thresholding(img, window_size=3, k=0.25, r=0.5, p=2, q=10):
    ...
    for j in range(img_height):
        for i in range(img_width):
            window = img[window_size*(j//window_size):window_size*(j//window_size) + window_size,
                         window_size*(i//window_size):window_size*(i//window_size) + window_size]
            # image is normalized
            window = window/255
            mean = np.mean(window)
            std_dev = np.std(window)
            local_threshold = (mean*(1 + (p*math.exp(-q*mean)) + (k*(((std_dev)/r) - 1))))
            if img[j][i]/255 < local_threshold:
                result[j][i] = 255
    ...
\end{lstlisting}

\subsubsection{Contrast Thresholding}
This solution analyzes de distance of the pixel value to the min and max values of the local neighborhood. If it is closer to the max, it is part of the object. It is background, otherwise. Listing \ref{code:contrast} clarifies the implementation.

\begin{lstlisting}[language=Python, caption={Contrast Thresholding Implementation}, label={code:contrast}]
  def contrast_local_thresholding(img, window_size=3):
    ...
    for j in range(img_height):
        for i in range(img_width):
            window = img[window_size*(j//window_size):window_size*(j//window_size) + window_size,
                         window_size*(i//window_size):window_size*(i//window_size) + window_size]
            min_v = np.min(window)
            max_v = np.max(window)
            dist_min = abs(min_v - int(img[j][i]))
            dist_max = abs(max_v - int(img[j][i]))
            if dist_min < dist_max:
                result[j][i] = 255
    ...
\end{lstlisting}

\subsubsection{Mean Thresholding}
This approach sets the threshold as the mean of the local neighborhood of the analyzed pixel. Listing \ref{code:mean} shows the main aspects of the implementation.

\begin{lstlisting}[language=Python, caption={Mean Thresholding Implementation}, label={code:mean}]
  def mean_local_thresholding(img, window_size=3):
    ...
    for j in range(img_height):
        for i in range(img_width):
            window = img[window_size*(j//window_size):window_size*(j//window_size) + window_size,
                         window_size*(i//window_size):window_size*(i//window_size) + window_size]
            mean = np.mean(window)
            if img[j][i] < mean:
                result[j][i] = 255
    ...
\end{lstlisting}

\subsubsection{Median Thresholding}
The solution sets the threshold as the median of the local neighborhood of the analyzed pixel. Listing \ref{code:median} shows main parts of implementation.

\begin{lstlisting}[language=Python, caption={Median Thresholding Implementation}, label={code:median}]
  def median_local_thresholding(img, window_size=3):
    ...
    for j in range(img_height):
        for i in range(img_width):
            window = img[window_size*(j//window_size):window_size*(j//window_size) + window_size,
                         window_size*(i//window_size):window_size*(i//window_size) + window_size]
            median = np.median(window)
            if img[j][i] < median:
                result[j][i] = 255
    ...
\end{lstlisting}

\section{Experiments}
The \textit{src/main.py} file executes the test pipeline. The idea is the following: for each input image, for each global threshold, execute the global thresholding method. For each window size, executes all local thresholding methods listed prior. For each global and local executions, save the output images and their respective histograms. Also, print in the standard output the ratio of black (object) pixels. The input images are stored in \textbf{input} folder as mentioned before, and their names and dimensions are listed in table \ref{table:input}. The input images are shown in figure \ref{fig:input-images}. The generated output images for the report are saved in $png$ extension in order to make it easier the report manipulation, and are stored in the report folder.

\begin{table}[h!]
\centering
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Image Names & Dimensions (width x height) \\
 \hline
  baboon.pgm &  512 x 512\\ 
 \hline
  fiducial.pgm & 640 x 480\\
 \hline
  monarch.pgm &  768 x 512\\ 
 \hline
  peppers.pgm & 623 x 594\\
 \hline
 retina.pgm & 256 x 256\\
 \hline
 sonnet.pgm & 384 x 510\\
 \hline
 wedge.pgm & 507 x 384\\
 \hline
\end{tabular}
\caption{Input images used in experiments}
\label{table:input}
\end{center}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.2\hsize]{../output/baboon.png}
  \includegraphics[width=0.2\hsize]{../output/fiducial.png}
  \includegraphics[width=0.2\hsize]{../output/monarch.png}
  \includegraphics[width=0.2\hsize]{../output/peppers.png}
  \includegraphics[width=0.2\hsize]{../output/retina.png}
  \includegraphics[width=0.2\hsize]{../output/sonnet.png}
  \includegraphics[width=0.2\hsize]{../output/wedge.png}
  \caption{Input images used in experiments. a) Baboon. b) Fiducial c) Monarch d) Peppers e) Retina f) Sonnet g) Wedge}
  \label{fig:input-images}
\end{figure}

The images original histogram are also shown in Figure \ref{fig:original-histograms}. They provide a good description of the input images. The Baboon image, which focus on the face of the animal, has a good number of details. The Fiducial image has a "almost binarized" appearance, with values concentrated in the extremes (either black or white, with high concentration of white), and also has some shading (as seen in intermediate values of the histogram). The monarch image has a concentration of intensities in the first half of the spectrum ($[0, 128]$), and it might be difficult to distinguish the butterfly from the background. The peppers image has good, and it might be difficult to separate objects from background. The retina image also has a high concentration of intensities in the first half of the spectrum. The sonnet one has variety of intensities, and some bad lighting in the text. The wedge image has high occurence of middle intensities.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\hsize]{../output/original-histogram-baboon.png}
  \includegraphics[width=0.3\hsize]{../output/original-histogram-fiducial.png}
  \includegraphics[width=0.3\hsize]{../output/original-histogram-monarch.png}
  \includegraphics[width=0.3\hsize]{../output/original-histogram-peppers.png}
  \includegraphics[width=0.3\hsize]{../output/original-histogram-retina.png}
  \includegraphics[width=0.3\hsize]{../output/original-histogram-sonnet.png}
  \includegraphics[width=0.3\hsize]{../output/original-histogram-wedge.png}
  \caption{Histogram of input images used in experiments. a) Baboon. b) Fiducial c) Monarch d) Peppers e) Retina f) Sonnet g) Wedge}
  \label{fig:original-histograms}
\end{figure}

For the global thresholding method, values in the $[0,255]$ scale were chosen. Table \ref{table:globalth} lists the global threshold valus used.

\begin{table}[h!]
  \centering
  \begin{center}
  \begin{tabular}{ |c| } 
   \hline
   Global Threshold values \\
   \hline
      50\\ 
   \hline
      128\\
   \hline
      200\\
    \hline
  \end{tabular}
  \caption{Global threshold values used in experiments}
  \label{table:globalth}
  \end{center}
  \end{table}

For the window sizes, a variety of values were picked. The neighborhood is chosen to be always a squared $nxn$ region, where n is the given size.
Table \ref{table:windows} lists the picked values.

\begin{table}[h!]
  \centering
  \begin{center}
  \begin{tabular}{ |c| } 
   \hline
   Window sizes (pixels)\\
   \hline
      3x3\\ 
   \hline
      9x9\\
   \hline
      15x15\\
    \hline
    33x33\\
    \hline
    99x99\\
    \hline
  \end{tabular}
  \caption{Window sizes for selecting pixel's neighborhood}
  \label{table:windows}
  \end{center}
  \end{table}

As we have 7 input images, 3 global threshold methods, 7 local threshold methods and 5 window sizes, we have 266 images of output, and their respective histograms. The output images are stored and well organized in the \textbf{output} folder.

\section{Discussion}
This section is organized in four parts. The first part analyses global thresholding methods and their effectiveness. The second part takes into consideration each local thresholding method, varying the window size. The third one does the comparisons between the local thresholding methods. To sum up, the comparison between global and local methods.

\subsection{Global threshold techniques}
In order to analyze the output images for the global threshold techniques, the following images are shown, comparing the thresholded images with different threshold values. The figures \ref{fig:global-fiducial}, \ref{fig:global-monarch}, \ref{fig:global-sonnet} shows the output images for the threshold values ($50$, $128$, $200$).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\hsize]{images/global-thresholding/50-threshold/fiducial.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/128-threshold/fiducial.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/200-threshold/fiducial.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/50-threshold/fiducialhistogram.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/128-threshold/fiducialhistogram.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/200-threshold/fiducialhistogram.png}
  \caption{Fiducial images with global threshold applying a) Global threshold equals 50. b) Global threshold equals 128. c) Global threshold equals 200. d)Histogram for value 50 e)Histogram for value 128 f)Histogram for value 200}
  \label{fig:global-fiducial}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\hsize]{images/global-thresholding/50-threshold/monarch.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/128-threshold/monarch.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/200-threshold/monarch.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/50-threshold/monarchhistogram.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/128-threshold/monarchhistogram.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/200-threshold/monarchhistogram.png}
  \caption{Monarch images with global threshold applying a) Global threshold equals 50. b) Global threshold equals 128. c) Global threshold equals 200. d)Histogram for value 50 e)Histogram for value 128 f)Histogram for value 200}
  \label{fig:global-monarch}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\hsize]{images/global-thresholding/50-threshold/sonnet.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/128-threshold/sonnet.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/200-threshold/sonnet.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/50-threshold/sonnethistogram.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/128-threshold/sonnethistogram.png}
  \includegraphics[width=0.3\hsize]{images/global-thresholding/200-threshold/sonnethistogram.png}
  \caption{Sonnet images with global threshold applying a) Global threshold equals 50. b) Global threshold equals 128. c) Global threshold equals 200. d)Histogram for value 50 e)Histogram for value 128 f)Histogram for value 200}
  \label{fig:global-sonnet}
\end{figure}

As expected, even though the global method is able to distinguish background from objects, it does not perform well for any of the thresholds and input images (except for 128 threshold with monarch images). It is does not take into consideration local variations of details and lightning, it cannot identify situations such as shading (for fiducial and sonnet) and any of the threshold values is good enough to separate the text from the background for the sonnet one. As it can be seen from the respective histograms, the number of pixels classified as objects increases as the threshold value is increased. This might not be adequate for images in general.
% It is interesting to notice, from those images, that the output images are closely related to the input images, even though it is used only the maximum and minimum values of grayscale and color channels. The error diffusion works well in applying the feeling of having multiple intensities of colors, notwithstanding we can still see points or regions with points in the output (\textit{e.g.:} monalisa (grayscale and colored), baboon (grayscale), peppers (colored)).  

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../input/monalisa_colored.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_stevenson-arce_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_colored_sierra_left-to-right.png}
%   \caption{Monalisa images manipulated in experiments. a) Input. b) grayscale by using Stevenson and Arce, alternate c) Colored by using Sierra, left to right}
%   \label{fig:effect-monalisa}
% \end{figure}

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../input/peppers_colored.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_grayscale_stucki_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_burkes_alternate.png}
%   \caption{Peppers images manipulated in experiments. a) Input. b) Grayscale by using Stucki, left to right c) Colored by using Burkes, alternate }
%   \label{fig:effect-peppers}
% \end{figure}

% Furthermore, the output images maintain the contrast seen in the input ones. It is clear that the error diffusion generates outputs which differs a lot of the process of binarization, in which necessary information of image is lost from the quantization according to a specific threshold. 

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../input/watch_colored.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_jarvis-judice-ninke_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_colored_floyd-steinberg_alternate.png}
%   \caption{Watch images manipulated in experiments. a) Input. b) Grayscale by using Jarvis, Judice and Ninke, left to right. c) Colored by using Floyd and Steinberg, alternate. }
%   \label{fig:effect-watch}
% \end{figure}

% \subsection{Sweeping mode differences}

% For sweeping mode comparisons, an ideia is to place side by side output images related to the same input and same error diffusion method, and comparing them visually. As the report does not include the images in its real dimensions, it is encouraged to the reader to access the images in the \textbf{output} folder for further inspection. Figures \ref{fig:sweep-baboon}, \ref{fig:sweep-monalisa}, \ref{fig:sweep-peppers} and \ref{fig:sweep-watch} shows 4 output images for every input image, created using the two sweep methods, in grayscale and color.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/baboon_grayscale_floyd-steinberg_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_grayscale_floyd-steinberg_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_floyd-steinberg_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_floyd-steinberg_alternate.png}
%   \caption{Baboon images manipulated in experiments. a) Grayscale by using Floyd and Steinberg, left to right. b) Grayscale by using Floyd and Steinberg, alternate. c) Colored by using Floyd and Steinberg, left to right. d) Colored by using Floyd and Steinberg, alternate.}
%   \label{fig:sweep-baboon}
% \end{figure}

% The differences are subtle, which makes it difficult to notice in the report, but by inspecting the original ones it is possible to see, for the baboon image input, that the sweep order from left to right creates more visible error points (\textit{e.g.: }yellow and green\textit{-ish} points are bigger in the nose of the baboon) than the alternate sweep. For their grayscale versions, the differences are harder to notice but the same effect is observable (if image is \textit{zoomed-in}).

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_stucki_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_stucki_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_colored_stucki_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_colored_stucki_alternate.png}
%   \caption{Monalisa images manipulated in experiments. a) Grayscale by using Stucki, left to right. b) Grayscale by using Stucki, alternate. c) Colored by using Stucki, left to right. d) Colored by using Stucki, alternate.}
%   \label{fig:sweep-monalisa}
% \end{figure}

% As the monalisa image input is smaller compared to the other ones, it is difficult to spot any differences between the sweep methods.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/peppers_grayscale_stevenson-arce_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_grayscale_stevenson-arce_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_stevenson-arce_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_stevenson-arce_alternate.png}
%   \caption{Peppers images manipulated in experiments. a) Grayscale by using Stevenson and Arce, left to right. b) Grayscale by using Stevenson and Arce, alternate. c) Colored by using Stevenson and Arce, left to right. d) Colored by using Stevenson and Arce, alternate.}
%   \label{fig:sweep-peppers}
% \end{figure}

% In the peppers case, the differences are more visible in the colored examples. As the background is black, the error diffusion is clearly spotted in the bottom of the image (green blur). For the left to right sweep, the blur are is bigger than in the alternate sweep, and it has the "bottom-to-right" direction, as expected from the mask characteristics and the sweep order. For the alternate sweep, the blur area is more "self-contained", as the propagation happens from left to right and right to left, so the error is diffused to the boundaries of the pepper itself, giving the feeling of more correctude. The error propagation is also clearer in this image for the grayscale cases, as we can see from the white spots in the black background.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_burkes_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_burkes_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_colored_burkes_left-to-right.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_colored_burkes_alternate.png}
%   \caption{Watch images manipulated in experiments. a) Grayscale by using Burkes, left to right. b) Grayscale by using Burkes, alternate. c) Colored by using Burkes, left to right. d) Colored by using Burkes, alternate.}
%   \label{fig:sweep-watch}
% \end{figure}

% The same effects noticed in the peppers image appears here for the colored output. Some error is spotted in the "Z arrow" in the image for left to right sweep, while the same error does not appear in the alternate one. Surprisingly, for the grayscale output and Burkes mask, the written text in the left-to-right image has better quality than in the alternate one.

% In the majority of cases, the alternate sweep obtained greater quality results, as it tries to compensate the error propagation to both directions.

% \subsection{Error Diffusion Methods}
% When analyzing the approaches listed in the literature to solve the problem, it is interesting to analyze them side-by-side. To do so, it is chosen colored or grayscale versions of a specific output, and a fixed sweep method, in this case, alternate. Then all solution proposals are grouped. Figures \ref{fig:methods-baboon}, \ref{fig:methods-monalisa}, \ref{fig:methods-peppers} and \ref{fig:methods-watch} shows 6 output images for each input, alternating the error diffusion methods.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_floyd-steinberg_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_stevenson-arce_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_burkes_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_sierra_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_stucki_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/baboon_colored_jarvis-judice-ninke_alternate.png}
%   \caption{Baboon images manipulated in experiments. a) Colored by using Floyd and Steiberg, alternate. b) Colored by using Stevenson and Arce, alternate. c) Colored by using Burkes, alternate. d) Colored by using Sierra, alternate. e) Colored by using Stucki, alternate. f) Colored by using Jarvis, Judice and Ninke, alternate. }
%   \label{fig:methods-baboon}
% \end{figure}

% For all error diffusion approaches, the main objective is achieved, but each of them have their peculiarity. With Floyd and Steinberg method, we can see the error more clearly as color points that don't fit to their background (yellow and green points in baboon nose, red points in its blue face). Stevenson and Arce highlights the contours, while we have the similar visibility of errors of Floyd and Steinberg. For Burkes, the error is less noticeable, and the contours of baboon face are atenuated a bit, as the mask is short. For Stucki, Sierra and Jarvis, Judice and Ninke, the visibility of errors is also reduced, and the contours are more visible, as the mask is bigger.   

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_floyd-steinberg_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_stevenson-arce_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_burkes_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_sierra_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_stucki_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/monalisa_grayscale_jarvis-judice-ninke_alternate.png}
%   \caption{Monalisa images manipulated in experiments. a) Grayscale by using Floyd and Steiberg, alternate. b) Grayscale by using Stevenson and Arce, alternate. c) Grayscale by using Burkes, alternate. d) Grayscale by using Sierra, alternate. e) Grayscale by using Stucki, alternate. f) Grayscale by using Jarvis, Judice and Ninke, alternate. }
%   \label{fig:methods-monalisa}
% \end{figure}

% For Monalisa, as the image is smaller, the differences are more difficult to grasp. But is is observable that the Floyd and Steinberg image has more noise than the other ones, and the Stevenson and Arce gives the feeling of having greater contrast. 

% The pepper output images have the same characteristics of the baboon image, already listed.

% For the watch images, all of them reproduces the image well, being able to distinguish their details, even though the noise feeling is greater in the Floyd and Steinberg approach. All of them fail to reproduce the black background correctly. 

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_floyd-steinberg_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_stevenson-arce_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_burkes_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_sierra_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_stucki_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/peppers_colored_jarvis-judice-ninke_alternate.png}
%   \caption{Peppers images manipulated in experiments. a) Colored by using Floyd and Steiberg, alternate. b) Colored by using Stevenson and Arce, alternate. c) Colored by using Burkes, alternate. d) Colored by using Sierra, alternate. e) Colored by using Stucki, alternate. f) Colored by using Jarvis, Judice and Ninke, alternate. }
%   \label{fig:methods-peppers}
% \end{figure}

 

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_floyd-steinberg_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_stevenson-arce_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_burkes_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_sierra_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_stucki_alternate.png}
%   \includegraphics[width=0.4\hsize]{../output/watch_grayscale_jarvis-judice-ninke_alternate.png}
%   \caption{Watch images manipulated in experiments. a) Grayscale by using Floyd and Steiberg, alternate. b) Grayscale by using Stevenson and Arce, alternate. c) Grayscale by using Burkes, alternate. d) Grayscale by using Sierra, alternate. e) Grayscale by using Stucki, alternate. f) Grayscale by using Jarvis, Judice and Ninke, alternate.}
%   \label{fig:methods-watch}
% \end{figure}

% \subsection{Pros and Cons}
% All methods achieve the objective of representing the original image characteristics with only two intensities per channel, each one having its own peculiarity. Floyd and Steinberg images have the least visible quality, as the propagated error is visible in the output, giving the feeling of being \textit{noisy}. Stevenson and Arce images are very interesting, as they highlights the contours of objects in the image. The other methods proposed generate images with great quality, not interfering a lot in its original contours. In general, the technique performs bad when propagating error to uniform spaces, as the blackbackgrounds in our experiments.

% Enabling the benchmarking flag, it was able to check the execution time for every output image. For each image channel, the execution time varied from 0.7 seconds for the monalisa image (smallest dimensions) to 9 seconds for the watch image (biggest dimensions), executing in a Intel(R) Core(TM) i7-4720HQ CPU @ 2.60GHz processor. As the technique sweeps through every pixel to do the error diffusion, the execution time is penalyzed.


% \section{Conclusion}
% The application of halftoning techniques to generate output images with similar characteristics of their inputs with only two levels of color intensity per channel was possible thanks to error propagation through image sweeping.

% The usage of different input images, error diffusion techniques and sweep modes made it possible to analyze each approach's strength.

\bibliography{ref-project} 
\bibliographystyle{ieeetr}

\end{document}
